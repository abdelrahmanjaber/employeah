{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91586644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17011458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "dataset_path = \"/Users/alessandromolinarroet/Desktop/web_application/Scapers/arbeitnow_jobs.csv\"\n",
    "skill_file = \"\"\n",
    "\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generative model (a bit slow + not very good + only english) \n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4317e8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Python - SQL - Docker - Git - Cloud - Agile - Experience in Python - Python - Backend engineering\n"
     ]
    }
   ],
   "source": [
    "job_description = \"\"\"\n",
    "We are hiring a Python backend engineer with experience in APIs, SQL, Docker,\n",
    "Git, cloud services (AWS preferred), and Agile methodologies.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Extract the list of required skills from this job description:\\n{job_description}\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=256)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6612f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“company rest days”) Wellpass membership and access to Jobrad, for your health Additional voluntary pension contribution and KITA subsidy, for you and your family’s future Competitive salary & meaningful equity stake, so you are invested in our success Tacto is an equal opportunities employer. We are committed to equal employment opportunity regardless of race, religion, sexual orientation, age, marital status, disability or gender identity. Please do not submit personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, data concerning your health, or data concerning your sexual orientation.\n",
      "Your Impact As part of Solution Engineering at Tacto, you'll bridge our powerful platform with measurable customer value through data expertise. Working directly with customers, you'll understand their unique supply chain challenges and implement tailored technical solutions. By integrating customer procurement data and configuring the platform to match their processes, you'll drive adoption and showcase immediate ROI. This technical role combines data engineering with customer interaction – you'll not only build robust data pipelines but also guide customers through implementation, helping transform our technology into business impact for Europe's industrial backbone. Your work will directly influence customer success, product evolution, and our company's growth trajectory. Your Tasks Data Integration & Pipeline Development: Design and implement stable, scalable pipelines that transform customer procurement data into actionable insights within Tacto's platform. Technical Onboarding Leadership: Drive the technical implementation process from kickoff to completion, working with customer IT teams to ensure smooth integration and rapid time-to-value. Solution Configuration: Customize the Tacto platform to match each customer's unique requirements, applying best practices in procurement and supplier management. Cross-Functional Collaboration: Work closely with Customer Success, Product, and Engineering to deliver integrated solutions and feed customer insights back into our product roadmap. Continuous Improvement: Develop reusable components and standardized approaches that increase onboarding efficiency while maintaining quality and customer satisfaction. Become AI Native: Proactively identify opportunities to leverage AI to accelerate your work, experiment with new approaches, and share successful workflows – contributing to Tacto's culture of high-velocity execution and innovation. Your Profile Now you may wonder what experiences and skills you need for this role. We believe that problem-solving, creativity, and drive are more important than tools that can be picked up. However, the following references will give a guideline of what experiences we think might be helpful. You enjoy getting to know customers, understanding their needs and achieving results together with an interdisciplinary team You have\n",
      "an interdisciplinary team You have experience in programming with SQL You are familiar with importing, cleansing, validating, transforming and exploring data You have worked with BI tools such as PowerBI or Tableau You have strong communication skills in both German and English Who We Are Tacto is an AI-driven procurement platform. Our mission is to strengthen Europe’s industrial “Mittelstand” by future proofing their supply chains. We streamline how companies buy, negotiate and manage procurement – we help them optimize their by far largest cost driver to stay competitive in the long-run. Trusted by hundreds of companies, Tacto transforms procurement into a strategic advantage. Based in Munich, our team of builders, entrepreneurs and industry experts is creating technology that truly matters. We’re backed by some of the best investors in the world including Sequoia Capital, Index Ventures, Cherry Ventures, Visionaries Club, UVC Partners as well as the founders of Personio, GetYourGuide, Helsing & Forto. Why It Matters The industrial Mittelstand – 90,000+ small and medium-sized enterprises – forms Europe’s economic backbone, driving 25% of Germany’s GDP and providing over 30% of jobs. These hidden champions manufacture the physical world around us, from the machines that build our cities to the medical devices that save lives. Today, a world of growing uncertainty and disruption is threatening their position. To stay competitive, the Mittelstand must unlock new efficiencies – and procurement is their largest, strategic lever to do so. At Tacto, we deliver tangible cost savings and strategic resilience at a time when it’s needed most. Our work extends beyond technology – it’s about protecting the foundation of Europe’s economy for generations to come. What We Offer An environment where we enable you to have an impact and where we focus on outcomes, not time-tracking A vibrant in-office culture in a beautiful office (at least 3 days/week) 26 + 4 vacation days per year (4\n",
      "vacation days per year (4 fixed “company rest days”) Wellpass membership\n"
     ]
    }
   ],
   "source": [
    "## Prototype chuking + similiraty (could be good to reduce size of description if done better)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=20):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = words[i:i+chunk_size]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "SKILL_LIST = [ \n",
    "    \"hard skills\", \"requirments\" \"name of skill\", \"your profile\"\n",
    "    ]\n",
    "\n",
    "def similarity_chunks(text, similarity=SKILL_LIST, levels=5, top_k=3):\n",
    "\n",
    "    chunks = chunk_text(text, chunk_size=5000, overlap=50)\n",
    "    skill_vec = model.encode(similarity, normalize_embeddings=True)\n",
    "    \n",
    "    for level in range(levels):\n",
    "\n",
    "        embeddings = model.encode(chunks, normalize_embeddings=True)\n",
    "        sims = util.cos_sim(skill_vec, embeddings)  \n",
    "        sims = sims.max(dim=0).values   \n",
    "        top_idx = sims.argsort(descending=True)[:top_k]\n",
    "        chunks = [chunks[i] for i in top_idx]\n",
    "\n",
    "        initial_chunk = 5000\n",
    "        min_chunk = 100\n",
    "        decay = 0.5\n",
    "        new_chunk_size = max(int(initial_chunk * (decay ** level)), min_chunk)\n",
    "\n",
    "        refined = []\n",
    "        for c in chunks:\n",
    "            refined.extend(chunk_text(c, chunk_size=new_chunk_size, overlap=5))\n",
    "\n",
    "        chunks = refined\n",
    "    return chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    jd = \"\"\"Your Impact\n",
    "\n",
    "As part of Solution Engineering at Tacto, you'll bridge our powerful platform with measurable customer value through data expertise. Working directly with customers, you'll understand their unique supply chain challenges and implement tailored technical solutions. By integrating customer procurement data and configuring the platform to match their processes, you'll drive adoption and showcase immediate ROI. This technical role combines data engineering with customer interaction – you'll not only build robust data pipelines but also guide customers through implementation, helping transform our technology into business impact for Europe's industrial backbone. Your work will directly influence customer success, product evolution, and our company's growth trajectory.\n",
    "\n",
    "Your Tasks\n",
    "\n",
    "Data Integration & Pipeline Development: Design and implement stable, scalable pipelines that transform customer procurement data into actionable insights within Tacto's platform.\n",
    "Technical Onboarding Leadership: Drive the technical implementation process from kickoff to completion, working with customer IT teams to ensure smooth integration and rapid time-to-value.\n",
    "Solution Configuration: Customize the Tacto platform to match each customer's unique requirements, applying best practices in procurement and supplier management.\n",
    "Cross-Functional Collaboration: Work closely with Customer Success, Product, and Engineering to deliver integrated solutions and feed customer insights back into our product roadmap.\n",
    "Continuous Improvement: Develop reusable components and standardized approaches that increase onboarding efficiency while maintaining quality and customer satisfaction.\n",
    "Become AI Native: Proactively identify opportunities to leverage AI to accelerate your work, experiment with new approaches, and share successful workflows – contributing to Tacto's culture of high-velocity execution and innovation.\n",
    "\n",
    "Your Profile\n",
    "\n",
    "Now you may wonder what experiences and skills you need for this role. We believe that problem-solving, creativity, and drive are more important than tools that can be picked up. However, the following references will give a guideline of what experiences we think might be helpful.\n",
    "\n",
    "You enjoy getting to know customers, understanding their needs and achieving results together with an interdisciplinary team\n",
    "You have experience in programming with SQL\n",
    "You are familiar with importing, cleansing, validating, transforming and exploring data\n",
    "You have worked with BI tools such as PowerBI or Tableau\n",
    "You have strong communication skills in both German and English\n",
    "\n",
    "Who We Are\n",
    "\n",
    "Tacto is an AI-driven procurement platform. Our mission is to strengthen Europe’s industrial “Mittelstand” by future proofing their supply chains. We streamline how companies buy, negotiate and manage procurement – we help them optimize their by far largest cost driver to stay competitive in the long-run. Trusted by hundreds of companies, Tacto transforms procurement into a strategic advantage.\n",
    "\n",
    "Based in Munich, our team of builders, entrepreneurs and industry experts is creating technology that truly matters. We’re backed by some of the best investors in the world including Sequoia Capital, Index Ventures, Cherry Ventures, Visionaries Club, UVC Partners as well as the founders of Personio, GetYourGuide, Helsing & Forto.\n",
    "\n",
    "Why It Matters\n",
    "\n",
    "The industrial Mittelstand – 90,000+ small and medium-sized enterprises – forms Europe’s economic backbone, driving 25% of Germany’s GDP and providing over 30% of jobs. These hidden champions manufacture the physical world around us, from the machines that build our cities to the medical devices that save lives.\n",
    "\n",
    "Today, a world of growing uncertainty and disruption is threatening their position. To stay competitive, the Mittelstand must unlock new efficiencies – and procurement is their largest, strategic lever to do so.\n",
    "\n",
    "At Tacto, we deliver tangible cost savings and strategic resilience at a time when it’s needed most. Our work extends beyond technology – it’s about protecting the foundation of Europe’s economy for generations to come.\n",
    "\n",
    "What We Offer\n",
    "\n",
    "An environment where we enable you to have an impact and where we focus on outcomes, not time-tracking\n",
    "A vibrant in-office culture in a beautiful office (at least 3 days/week)\n",
    "26 + 4 vacation days per year (4 fixed “company rest days”)\n",
    "Wellpass membership and access to Jobrad, for your health\n",
    "Additional voluntary pension contribution and KITA subsidy, for you and your family’s future\n",
    "Competitive salary & meaningful equity stake, so you are invested in our success\n",
    "\n",
    "Tacto is an equal opportunities employer. We are committed to equal employment opportunity regardless of race, religion, sexual orientation, age, marital status, disability or gender identity. Please do not submit personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, data concerning your health, or data concerning your sexual orientation.\n",
    "\"\"\"\n",
    "    chunks = similarity_chunks(jd)\n",
    "    for c in chunks:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc32674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 75.36 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 84.50 examples/s]\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/ks/r1n0ckqj7klc419d9ryqdxz40000gn/T/ipykernel_916/641266684.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA adapter...\n",
      "trainable params: 152,839 || all params: 66,521,102 || trainable%: 0.2298\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.746625</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.657096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>1.587803</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>1.543345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.654700</td>\n",
       "      <td>1.524694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/web_app/lib/python3.11/site-packages/transformers/trainer.py:4380: UserWarning: mtime may not be reliable on this filesystem, falling back to numerical ordering\n",
      "  warnings.warn(\"mtime may not be reliable on this filesystem, falling back to numerical ordering\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./distilbert-skill-extraction-final\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Results:\n",
      "eval_loss: 1.5878\n",
      "eval_precision: 0.2000\n",
      "eval_recall: 0.2000\n",
      "eval_f1: 0.2000\n",
      "eval_runtime: 0.0564\n",
      "eval_samples_per_second: 17.7380\n",
      "eval_steps_per_second: 17.7380\n",
      "epoch: 5.0000\n"
     ]
    }
   ],
   "source": [
    "# Fine tune very small model + adapter\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score \n",
    "model_name = \"distilbert-base-uncased\"\n",
    "use_lora = True\n",
    "\n",
    "label_list = [\"O\", \"B-SOFT\", \"I-SOFT\", \"B-HARD\", \"I-HARD\", \"B-KNOW\", \"I-KNOW\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "dataset = [\n",
    "    {\n",
    "        \"tokens\": [\"We\", \"are\", \"seeking\", \"a\", \"data\", \"analyst\", \"with\", \"strong\", \"Python\", \"and\", \"SQL\", \"skills\", \"and\", \"at\", \"least\", \"3\", \"years\", \"of\", \"experience\", \".\"],\n",
    "        \"ner_tags\": [\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-SOFT\",\"B-HARD\",\"O\",\"B-HARD\",\"O\",\"O\",\"O\",\"O\",\"B-KNOW\",\"I-KNOW\",\"O\",\"I-KNOW\",\"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"The\", \"candidate\", \"must\", \"have\", \"excellent\", \"communication\", \"skills\", \"and\", \"experience\", \"with\", \"Docker\", \"and\", \"Kubernetes\", \".\"],\n",
    "        \"ner_tags\": [\"O\",\"O\",\"O\",\"O\",\"B-SOFT\",\"I-SOFT\",\"O\",\"O\",\"O\",\"O\",\"B-HARD\",\"O\",\"B-HARD\",\"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"A\", \"Master\", \"'\", \"s\", \"degree\", \"in\", \"Computer\", \"Science\", \"is\", \"required\", \"with\", \"knowledge\", \"of\", \"TensorFlow\", \"and\", \"PyTorch\", \".\"],\n",
    "        \"ner_tags\": [\"O\",\"B-KNOW\",\"I-KNOW\",\"I-KNOW\",\"I-KNOW\",\"O\",\"I-KNOW\",\"I-KNOW\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-HARD\",\"O\",\"B-HARD\",\"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"Looking\", \"for\", \"a\", \"project\", \"manager\", \"with\", \"strong\", \"leadership\", \"and\", \"problem-solving\", \"abilities\", \"and\", \"5\", \"+\", \"years\", \"managing\", \"technical\", \"teams\", \".\"],\n",
    "        \"ner_tags\": [\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-SOFT\",\"I-SOFT\",\"O\",\"B-SOFT\",\"I-SOFT\",\"O\",\"B-KNOW\",\"I-KNOW\",\"I-KNOW\",\"I-KNOW\",\"I-KNOW\",\"I-KNOW\",\"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"We\", \"need\", \"a\", \"backend\", \"engineer\", \"skilled\", \"in\", \"Node\", \".\", \"js\", \"and\", \"REST\", \"APIs\", \"with\", \"AWS\", \"experience\", \".\"],\n",
    "        \"ner_tags\": [\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-HARD\",\"I-HARD\",\"I-HARD\",\"O\",\"B-HARD\",\"I-HARD\",\"O\",\"B-HARD\",\"I-HARD\",\"O\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "for item in dataset:\n",
    "    item[\"ner_tags\"] = [label2id[tag] for tag in item[\"ner_tags\"]]\n",
    "\n",
    "train_data = dataset\n",
    "val_data = dataset[:1]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100) \n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx]) \n",
    "            else:\n",
    "                label_ids.append(-100) \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "if use_lora:\n",
    "    print(\"Applying LoRA adapter...\")\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.TOKEN_CLS,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_lin\", \"v_lin\"], \n",
    "        bias=\"none\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-skill-extraction\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4 if use_lora else 2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5, \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "output_path = \"./distilbert-skill-extraction-final\"\n",
    "if use_lora:\n",
    "    model.save_pretrained(output_path)\n",
    "else:\n",
    "    trainer.save_model(output_path)\n",
    "\n",
    "tokenizer.save_pretrained(output_path)\n",
    "print(f\"Model saved to {output_path}\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nFinal Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a721f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Loading tokenizer...\n",
      "Loading LoRA adapter from ./distilbert-skill-extraction-final...\n",
      "Model loaded successfully. Ready for inference!\n",
      "------------------------------\n",
      "Input: We need a strong leader with Python, teamwork, and 5 years of experience.\n",
      "\n",
      "Predicted Entities:\n",
      " - need: B-HARD\n",
      " - Python,: I-KNOW\n",
      " - years: I-KNOW\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "output_path = \"./distilbert-skill-extraction-final\"\n",
    "label_list = [\"O\", \"B-SOFT\", \"I-SOFT\", \"B-HARD\", \"I-HARD\", \"B-KNOW\", \"I-KNOW\"]\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_path)\n",
    "\n",
    "print(f\"Loading LoRA adapter from {output_path}...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    output_path\n",
    ")\n",
    "model.eval() \n",
    "\n",
    "print(\"Model loaded successfully. Ready for inference!\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "def predict_skills(text, model, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        text.split(),\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "    word_ids = inputs.word_ids()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits.argmax(dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    final_labels = []\n",
    "    \n",
    "    for token_id, word_idx in enumerate(word_ids):\n",
    "        if word_idx is not None and (word_idx == 0 or word_ids[token_id - 1] != word_idx):\n",
    "            label = id2label[predictions[token_id]]\n",
    "            final_labels.append((text.split()[word_idx], label))\n",
    "    entities = [(word, label) for word, label in final_labels if label != 'O']\n",
    "    \n",
    "    return entities\n",
    "\n",
    "test_sentence = \"We need a strong leader with Python, teamwork, and 5 years of experience.\"\n",
    "print(f\"Input: {test_sentence}\")\n",
    "\n",
    "results = predict_skills(test_sentence, model, tokenizer)\n",
    "\n",
    "print(\"\\nPredicted Entities:\")\n",
    "if results:\n",
    "    for word, label in results:\n",
    "        print(f\" - {word}: {label}\")\n",
    "else:\n",
    "    print(\"No skills or knowledge entities detected.\")\n",
    "\n",
    "print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
